{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Credits\n",
    "__This notebook has been adapted from__\n",
    "- https://git.skewed.de/count0/graph-tool/-/wikis/installation-instructions#googles-colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDn_lVxg3Z2G"
   },
   "source": [
    "# Installing graph-tool (https://graph-tool.skewed.de)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "To import a library that's not in Colaboratory by default, you can use `!apt-get install`.\n",
    "\n",
    "But since graph-tool is not in the official repository, we need to add it to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQ18Kd5F3uKe",
    "outputId": "379c8deb-7a09-4d45-cfc9-afc1e694dfdc"
   },
   "outputs": [],
   "source": [
    "!echo \"deb http://downloads.skewed.de/apt bionic main\" >> /etc/apt/sources.list\n",
    "!apt-key adv --keyserver keyserver.ubuntu.com --recv-key 612DEFB798507F25\n",
    "!apt-get update\n",
    "!apt-get install python3-graph-tool python3-matplotlib python3-cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pr-ut3scRrok",
    "outputId": "650030c1-9b2a-49cb-871a-42cbf2ec9de9"
   },
   "outputs": [],
   "source": [
    "#python3-cairo from Ubuntu's reposity is linked with a different python version; we need to improvise\n",
    "!apt purge python3-cairo\n",
    "!apt install libcairo2-dev pkg-config python3-dev\n",
    "!pip install --force-reinstall pycairo\n",
    "!pip install zstandard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZBU2HvQqJAw"
   },
   "source": [
    "# Repository of unweighted graphs: https://networks.skewed.de/?tags=Unweighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Hk9usv9pUBZ",
    "outputId": "cbc1290b-66b6-401f-9869-4269699e3add"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graph_tool.all as gt\n",
    "from google.colab import files\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "def export_all_unweightedGraphs_erodeCSV(model_names,download_file,store_on_gdrive,path_g_drive=\"unweightedGraphs/\",prefix_to_weight=dict()):\n",
    "   #Exports in ERODE CSV format all weighted graphs in the list model_names. \n",
    "   #The CSV file is either stored on google drive, or downloaded, as required by the parameters (Google drive has precedence on downloading)\n",
    "   #If the google drive option is chosen, the files will be stored in folder weightedGraphs, or different paths provided in parameter path_g_drive\n",
    "   #  This should be a relative path to a folder (ending with a slash). E.g., if you pass \"weightedGraphs/\", we will store files in it in MyDrive/weightedGraphs/\n",
    "\n",
    "\n",
    "  print('Processing and exporting models')\n",
    "  print(\"\",model_names)\n",
    "\n",
    "  #Prepare gdrive\n",
    "  gdrive_path=\"/content/drive/MyDrive/\"+path_g_drive\n",
    "  if store_on_gdrive:\n",
    "    drive.mount('/content/drive')\n",
    "    if not path.exists(gdrive_path):\n",
    "      os.mkdir(gdrive_path)\n",
    "    if not path.exists(gdrive_path+\"/csv/\"):\n",
    "      os.mkdir(gdrive_path+\"/csv/\")\n",
    "\n",
    "  #Export required weighted graps\n",
    "  for model_name in model_names:\n",
    "    weight_name='weight'\n",
    "    #for key in prefix_to_weight:\n",
    "    #  if model_name.startswith(key):\n",
    "    #    weight_name=prefix_to_weight[key]\n",
    "    #    break\n",
    "    export_weightedGraph_erodeCSV(model_name,download_file,store_on_gdrive,gdrive_path,weight_name)\n",
    "\n",
    "  #Close gdrive\n",
    "  if store_on_gdrive:\n",
    "    drive.flush_and_unmount()\n",
    "\n",
    "  print('All computations completed')\n",
    "\n",
    "def export_unweightedGraph_erodeCSV(model_name,download_file,store_on_gdrive,gdrive_path,weight_name):\n",
    "  #load model\n",
    "  print(\"PROCESSING:\",model_name,\"...\")\n",
    "\n",
    "  g = gt.collection.ns[model_name]\n",
    "\n",
    "  #if not(weight_name in g.edge_properties):\n",
    "  #  print(\" MODEL DROPPED BECAUSE NOT WEIGHTED\")\n",
    "  #  return\n",
    "  add_rev= not g.is_directed()\n",
    "    \n",
    "  max_nodes=200\n",
    "  n_nodes=g.num_vertices()\n",
    "  if n_nodes > max_nodes:\n",
    "    print(\" MODEL DROPPED BECAUSE IT HAS MORE THAN\",max_nodes,\"NODES\")\n",
    "    return        \n",
    "    \n",
    "  #n_nodes=len(g.get_edges())\n",
    "  #if add_rev:\n",
    "  #      n_edges=n_edges*2\n",
    "  #if n_edges > max_edges:\n",
    "  #  print(\" MODEL DROPPED BECAUSE IT HAS MORE THAN\",max_edges,\"EDGES\")\n",
    "  #  return        \n",
    "\n",
    "  model_name = model_name.replace(\"/\",\"_\")\n",
    "  model_name = model_name.replace(\" \",\"_\")\n",
    "  model_name = model_name.replace(\"-\",\"_\")\n",
    "  model_name = model_name.replace(\"(\",\"_\")\n",
    "  model_name = model_name.replace(\")\",\"_\")\n",
    "\n",
    "  #store unweighted edges in a dataframe, and add weights (1)\n",
    "  df = pd.DataFrame(data=g.get_edges(), columns=[\"source\", \"target\"])\n",
    "  df[\"source\"]=df[\"source\"]+1\n",
    "  df[\"target\"]=df[\"target\"]+1\n",
    "  \n",
    "  df['weight']=np.ones(len(df))\n",
    "  #print(\"BEWARE:\",model_name,\"DOES NOT HAVE WEIGHTS.\\n WE SET ALL WEIGHTS TO ONE!\")\n",
    "  model_name=model_name+\"_defaultWeights\"\n",
    "\n",
    "\n",
    "  #add_rev_str= str(add_rev)\n",
    "  if add_rev:\n",
    "      df_rev = df[[\"target\",\"source\",'weight']]\n",
    "      df_rev.columns = [\"source\", \"target\",'weight']\n",
    "      df=pd.concat([df,df_rev])\n",
    "      df.reset_index(drop=True, inplace=True)\n",
    "      add_rev=False\n",
    "      \n",
    "\n",
    "  #n_nodes=g.num_vertices()\n",
    "  df0 = pd.DataFrame([[n_nodes,n_nodes,n_nodes]], columns=[\"source\", \"target\",'weight'])\n",
    "  df=pd.concat([df0, df])\n",
    "  df.reset_index(drop=True, inplace=True)\n",
    "  #print(df)\n",
    "\n",
    "  dfB = pd.DataFrame(data=np.zeros(n_nodes), columns=[\"B\"])\n",
    "\n",
    "    \n",
    "  #compute file name of csv file\n",
    "  csv_file_name=\"csv/\"+model_name+'A.csv'\n",
    "  csv_file_nameB=\"csv/\"+model_name+'B.csv'\n",
    "  erode_file_name=\"import_\"+model_name+'.ode'\n",
    "    \n",
    "  if store_on_gdrive:\n",
    "    csv_file_name  =gdrive_path+\"csv/\"+model_name+'A.csv'\n",
    "    csv_file_nameB =gdrive_path+\"csv/\"+model_name+'B.csv'\n",
    "    erode_file_name=gdrive_path+\"import_\"+model_name+'.ode'\n",
    "    \n",
    "  \n",
    "\n",
    "  print(\"  \",\"Graph processed, and data structure created\")\n",
    "\n",
    "  #create csv file and download if required\n",
    "  df.to_csv( csv_file_name ,index=False,header=False)\n",
    "  dfB.to_csv(csv_file_nameB,index=False,header=False)\n",
    "\n",
    "  # Create ERODE file to import the CSV ones\n",
    "  with open(erode_file_name, 'w') as fp:\n",
    "    fp.write('begin model import_'+model_name+\"\\n\")\n",
    "    fp.write(' importAffineSystem(fileIn=\"csv/'+model_name+'A.csv\", BFile=\"csv/'+model_name+'B.csv\", ICFile=\"csv/'+model_name+'B.csv\", addReverseEdges=false)\\n')\n",
    "    #fp.write(' write(fileOut=\"ODE/'+model_name+'._ode\",format=ODE)\\n')\n",
    "    #fp.write(' reduceBE(reducedFile=\"BE/'+model_name+'_BE._ode\", csvFile=\"BE.csv\")\\n')\n",
    "    #fp.write(' reduceFE(reducedFile=\"FE/'+model_name+'_FE._ode\", csvFile=\"FE.csv\")\\n')\n",
    "    fp.write(' write(fileOut=\"EULER_ODE/'+model_name+'.ode\",format=EULER)\\n')\n",
    "    #fp.write(' reduceRndFME(aggregationFunction=TIMES,reducedFile=\"RndFME_user/'+model_name+'_RndFME_user._ode\", csvFile=\"RndTIMES_USER.csv,prePartition=USER\")\\n')\n",
    "    fp.write('end model\\n')\n",
    "    \n",
    "  \n",
    "  if download_file and not store_on_gdrive:\n",
    "    files.download(csv_file_name)\n",
    "    files.download(csv_file_nameB)\n",
    "    files.download(erode_file_name)\n",
    "\n",
    "  print(\"  \",\"All files exported\")\n",
    "\n",
    "    \n",
    "  \n",
    "#Choose the models to consider\n",
    "# Just https://networks.skewed.de/net/blumenau_drug\n",
    "#model_names=[\"blumenau_drug\"]\n",
    "#model_names=[\"cattle\"]\n",
    "#model_names=[\"blumenau_drug\",\"cattle\",\"train_terrorists\"]\n",
    "\n",
    "#All weighted networks on 21/07/2022 from smallest number of nodes to largest#\n",
    "# Taken from https://networks.skewed.de/?tags=Unweighted after clicking on 'Nodes'\n",
    "model_names=[\n",
    "    'sa_companies','florentine_families','high_tech_company','moreno_taro','november17','sp_baboons',\n",
    "    'dutch_school','zebras','7th_graders','karate','dutch_criticism','montreal','ceo_club','elite',\n",
    "    'macaque_neural','sp_kenyan_households','contiguous_usa','cs_department','dolphins','terrorists_911',\n",
    "    'law_firm','sp_hospital','reality_mining','swingers','polbooks','adjnoun','fresh_webs','sp_hypertext',\n",
    "    'football','football_tsevans','revolution','email_company','sp_high_school_new','foodweb_little_rock',\n",
    "    'student_cooperation','jazz_collab','interactome_pdz','physician_trust','contact','malaria_genes',\n",
    "    'sp_high_school','marvel_partnerships','kidnappings','facebook_friends','ugandan_village','sp_colocation',\n",
    "    'ecoli_transcription','eu_airlines','celegans_metabolic','wiki_science','internet_top_pop','copenhagen',\n",
    "    'yeast_transcription','uni_email','euroroad','faa_routes','crime','polblogs','collins_yeast','interactome_stelzl',\n",
    "    'interactome_yeast','plant_pol_robertson','kegg_metabolic','board_directors','dnc','interactome_figeys',\n",
    "    'us_air_traffic','urban_streets','interactome_vidal','power','facebook_organizations','fullerene_structures',\n",
    "    'jung','celegans_interactomes','reactome','foursquare','jdk','route_views','software_dependencies',\n",
    "    'sp_infectious','word_adjacency','dblp_cite','anybeat','chicago_road','google','genetic_multiplex',\n",
    "    'marvel_universe','internet_as','word_assoc','cora','movielens_100k','caida_as','digg_reply','nematode_mammal',\n",
    "    'linux','arxiv_citation','scotus_majority','topology','hiv_transmission','email_enron','inploid','pgp_strong',\n",
    "    'paris_transportation','slashdot_threads','python_dependency','gnutella','lkml_reply','marker_cafe',\n",
    "    'epinions_trust','twitter_15m','prosper','wiki_link_dyn','livemocha','ego_social','foursquare_friendships',\n",
    "    'arxiv_authors','dbpedia_writer','lastfm_aminer','wordnet','dbtropes_feature','douban','dbpedia_starring',\n",
    "    'github','dbpedia_recordlabel','dbpedia_producer','academia_edu','google_plus','flickr_aminer',\n",
    "    'dbpedia_location','dbpedia_occupation','email_eu','dbpedia_genre','discogs_label','stanford_web',\n",
    "    'notre_dame_web','corporate_directors','openstreetmap','lkml_thread','citeseer','amazon_copurchases',\n",
    "    'dblp_coauthor_snap','bookcrossing','twitter','flickr_groups','visualizeus','dbpedia_country','petster',\n",
    "    'stackoverflow','yahoo_ads','berkstan_web','myspace_aminer','citeulike','google_web','dbpedia_team',\n",
    "    'bibsonomy','reuters','lastfm','wikitree','hyves','trec_web','as_skitter','trec','discogs_genre',\n",
    "    'dblp_coauthor','wikipedia_growth','roadnet','discogs_affiliation','wiki_categories','baidu','twitter_events',\n",
    "    'flickr_growth','wikipedia-en-talk','flixster','qa_user','mag_history_coauthor','mag_geology_coauthor',\n",
    "    'wiki_talk','livejournal_aminer','us_patents','foursquare_global','dbpedia_all','livejournal','mislove_osn',\n",
    "    'twitter_sample','dblp_simplices','bitcoin','dblp_author_paper','wikipedia_link','dbpedia_link','us_roads',\n",
    "    'delicious','trackers','twitter_social','twitter_2009','soc_net_comms']\n",
    "\n",
    "#Unfortunately, not all models classified as weighted have weights. We handle these cases setting deafult weight 1\n",
    "#model_names=[\"hens\"] # DOES NOT HAVE WEIGHTS! The default '1' will be set to all edges\n",
    "\n",
    "# prefix_to_weight={'eu_procurements_alt/':'pctSingleBid','arxiv_collab/':'value','bag_of_words/':'count',\n",
    "#                   'budapest_connectome/':'occurences','celegans_2019':'connectivity',\n",
    "#                   'us_agencies/':'link_counts','plant_pol_vazquez':'count'}\n",
    "\n",
    "#Set flags on where to store the obtained CSV files\n",
    "download_file=False\n",
    "store_on_gdrive=True\n",
    "path_g_drive=\"weightedGraphs/\"\n",
    "export_all_unweightedGraphs_erodeCSV(model_names,download_file,store_on_gdrive,path_g_drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If interested in drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graph_tool.all import *\n",
    "# g = collection.data[\"celegansneural\"]\n",
    "# state = minimize_nested_blockmodel_dl(g)\n",
    "# state = minimize_nested_blockmodel_dl(g)\n",
    "# state.draw()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Graph_tool_to_ERODE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
